{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33a1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import helper\n",
    "import pandas as pd\n",
    "#from utils import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import pennylane as qml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce35c5",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7a535",
   "metadata": {},
   "source": [
    "Stock price prediction is one of the most rewarding problems in modern finance, where the accurate forecasting of future stock prices can yield significant profit and reduce the risks. LSTM (Long Short-Term Memory) is a recurrent Neural Network (RNN) applicable to a broad range of problems aiming to analyze or classify sequential data. Therefore, many people have used LSTM to predict the future stock price based on the historical data sequences with great success.\n",
    "\n",
    "On the other hand, recent studies have shown that the LSTM's efficiency and trainability can be improved by replacing some of the layers in the LSTM with variational quantum layers, thus making it a quantum-classical hybrid model of LSTM which we will call QLSTM for Quantum LSTM. In the study done by Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang, they show that QLSTM offers better trainability compared to its classical counterpart as it proved to learn significantly more information after the first training epoch than its classical counterpart, learnt the local features better, all while having a comparable number of parameters. Inspired by these recent results, we proceed to test this variational quantum-classical hybrid neural network technique on stock price predictions. \n",
    "\n",
    "In the following notebook, we show a proof of concept that QLSTM can be used to great effect for stock price prediction, offering comparable and arguably better results than its classical counterpart. To do so, we implement both LSTM and QLSTM to predict the stock prices of the company Merck & Co. Inc (MRK) with the same number of features, of which we chose based on earlier studies done with stock price predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922097fa",
   "metadata": {},
   "source": [
    "This submission was motivated by a combination of a few separate studies:\n",
    "\n",
    "Stock price prediction use BERT and GAN: https://arxiv.org/pdf/2107.09055.pdf, Priyank Sonkiya, Vikas Bajpai, Anukriti Bansal <br>\n",
    "Quantum Long Short-Term Memory: https://arxiv.org/pdf/2009.01783.pdf, Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang <br>\n",
    "\n",
    "With code and ideas reused and repurposed from the following sources:\n",
    "\n",
    "Example of a QLSTM: https://github.com/rdisipio/qlstm, Riccardo Di Sipio <br>\n",
    "How to use PyTorch LSTMs for time series regression: https://www.crosstab.io/articles/time-series-pytorch-lstm, Brian Kent <br>\n",
    "Using GANs to predict stock price movement: https://towardsdatascience.com/aifortrading-2edd6fac689d, Boris Banushev<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bb7d7",
   "metadata": {},
   "source": [
    "## Brief Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e301a55",
   "metadata": {},
   "source": [
    "To demonstrate the use of QLSTM for stock prediction, we use the stock prices of the company Merck & Co. Inc (MRK). The notebook will proceed in the following manner:\n",
    "\n",
    "1. Brief description of Data\n",
    "2. Using Classical LSTM to perform stock price prediction\n",
    "3. Defining QLSTM and using it to perform stock price prediction\n",
    "4. Comparison between LSTM and QLSTM\n",
    "\n",
    "Note that for the LSTM, we would be using PyTorch; while for the QLSTM we would be using PyTorch and Pennylane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f50219",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71976209",
   "metadata": {},
   "source": [
    "- weight decay coefficient (weight decay): 0, 0.0001, 0.01 \n",
    "- width of the layer (width)(controlled by hidden units): 16, 128, 512 \n",
    "- mini-batch size (batch size): 1, 5, 10\n",
    "- learning rate (learning rate):  0.001, 0.05, 0.1\n",
    "- dropout probability (dropout)(only classical for now): 0, 0.05, 0.3\n",
    "- depth of the architecture(depth): 1, 6, 12\n",
    "- sequence length: 3, 7, 30\n",
    "- num_epochs: 5, 10, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41647415",
   "metadata": {},
   "source": [
    "number of iterations = batch_size * num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef7a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sequence_length = 30\n",
    "dropout_value = 0\n",
    "num_layers = 1\n",
    "\n",
    "weight_decay_value = 0\n",
    "learning_rate = 0.05\n",
    "\n",
    "num_hidden_units = 16\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987a8135",
   "metadata": {},
   "source": [
    "- No. of Qubits: 4, 8, 12\n",
    "- Feature Map: RY + RZ, \n",
    "- Entanglement structure: Circular, Full\n",
    "- Rotation structure: Full 3\n",
    "- Number of layers: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc35fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4, # No. of Qubits:\n",
    "                n_qlayers=1, # Number of layers:\n",
    "                n_vrotations=3,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        #self.dev_forget = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_input = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_update = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_output = qml.device(self.backend, wires=self.n_qubits)\n",
    "        \n",
    "        def ansatz(params, wires_type):\n",
    "            # Entangling layer: Circular\n",
    "            for j in range(self.n_qubits):\n",
    "                if j < (self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[wires_type[j], wires_type[j + 1]])\n",
    "                else:\n",
    "                    qml.CNOT(wires=[wires_type[j], wires_type[0]])\n",
    "\n",
    "            # Variational layer: Full 3\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[0][i], wires=wires_type[i])\n",
    "                qml.RY(params[1][i], wires=wires_type[i])\n",
    "                qml.RZ(params[2][i], wires=wires_type[i])\n",
    "                \n",
    "        def VQC(features, weights, wires_type):\n",
    "            # Preproccess input data to encode the initial state.\n",
    "            #qml.templates.AngleEmbedding(features, wires=wires_type)\n",
    "            # Feature Map\n",
    "            ry_params = [torch.arctan(feature) for feature in features]\n",
    "            rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=wires_type[i])\n",
    "                qml.RY(ry_params[i], wires=wires_type[i])\n",
    "                qml.RZ(ry_params[i], wires=wires_type[i])\n",
    "        \n",
    "            #Variational block.\n",
    "            qml.layer(ansatz, self.n_qlayers, weights, wires_type = wires_type)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations, self.n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations, n_qubits) = ({self.n_qlayers}, {self.n_vrotations}, {self.n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            if self.batch_first is True:\n",
    "                x_t = x[:, t , :]\n",
    "            else:\n",
    "                x_t = x[t, : , :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490a96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372111d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf2258",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841734de",
   "metadata": {},
   "source": [
    "To briefly describe the data, we have collected the historical data of the MRK stock prices, of which we focus on the closing price. Our goal is then to forecast the closing stock prices of MRK using LSTM (or QLSTM). \n",
    "\n",
    "To achieve this goal, we have collected the data and information we thought necessary and important. This includes the following:\n",
    "\n",
    "- Technical indicators\n",
    "- Trend approximations (Fourier Transforms)\n",
    "- ARIMA \n",
    "- Correlated assets\n",
    "- Sentimental analysis\n",
    "\n",
    "While interesting and important in its own right, we have decided not to go into detail for the data collection in this notebook. For more information, please go take a look at the Data Collection notebook also in this Github. \n",
    "\n",
    "In this section, our goal is to process the data to get it ready for the LSTM and QLSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37e1ea",
   "metadata": {},
   "source": [
    "First, we read in the data, dropping the index and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c36071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ma7</th>\n",
       "      <th>ma21</th>\n",
       "      <th>26ema</th>\n",
       "      <th>12ema</th>\n",
       "      <th>MACD</th>\n",
       "      <th>20sd</th>\n",
       "      <th>upper_band</th>\n",
       "      <th>lower_band</th>\n",
       "      <th>...</th>\n",
       "      <th>GSK</th>\n",
       "      <th>LLY</th>\n",
       "      <th>NVS</th>\n",
       "      <th>NYSE</th>\n",
       "      <th>NASDAQ</th>\n",
       "      <th>FT3</th>\n",
       "      <th>FT6</th>\n",
       "      <th>FT9</th>\n",
       "      <th>ARIMA</th>\n",
       "      <th>Close_lead1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.011450</td>\n",
       "      <td>14227229</td>\n",
       "      <td>30.050436</td>\n",
       "      <td>29.200291</td>\n",
       "      <td>29.514006</td>\n",
       "      <td>29.867322</td>\n",
       "      <td>0.353316</td>\n",
       "      <td>0.835852</td>\n",
       "      <td>30.871996</td>\n",
       "      <td>27.528586</td>\n",
       "      <td>...</td>\n",
       "      <td>20.441219</td>\n",
       "      <td>22.546120</td>\n",
       "      <td>26.214659</td>\n",
       "      <td>6671.140137</td>\n",
       "      <td>2017.979980</td>\n",
       "      <td>55.454947</td>\n",
       "      <td>50.337802</td>\n",
       "      <td>47.947327</td>\n",
       "      <td>31.011450</td>\n",
       "      <td>31.440840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.440840</td>\n",
       "      <td>20081566</td>\n",
       "      <td>30.318975</td>\n",
       "      <td>29.299346</td>\n",
       "      <td>29.688906</td>\n",
       "      <td>30.115697</td>\n",
       "      <td>0.426791</td>\n",
       "      <td>0.963549</td>\n",
       "      <td>31.226444</td>\n",
       "      <td>27.372247</td>\n",
       "      <td>...</td>\n",
       "      <td>20.290920</td>\n",
       "      <td>22.639698</td>\n",
       "      <td>26.532934</td>\n",
       "      <td>6697.220215</td>\n",
       "      <td>2024.229980</td>\n",
       "      <td>55.359252</td>\n",
       "      <td>50.140845</td>\n",
       "      <td>47.659130</td>\n",
       "      <td>31.440840</td>\n",
       "      <td>31.183207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.183207</td>\n",
       "      <td>10438080</td>\n",
       "      <td>30.564340</td>\n",
       "      <td>29.420211</td>\n",
       "      <td>29.822316</td>\n",
       "      <td>30.283528</td>\n",
       "      <td>0.461212</td>\n",
       "      <td>1.026446</td>\n",
       "      <td>31.473102</td>\n",
       "      <td>27.367319</td>\n",
       "      <td>...</td>\n",
       "      <td>20.280550</td>\n",
       "      <td>22.492653</td>\n",
       "      <td>26.608170</td>\n",
       "      <td>6687.939941</td>\n",
       "      <td>2024.430054</td>\n",
       "      <td>55.263482</td>\n",
       "      <td>49.944470</td>\n",
       "      <td>47.372320</td>\n",
       "      <td>31.183207</td>\n",
       "      <td>31.364504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.364504</td>\n",
       "      <td>10302154</td>\n",
       "      <td>30.858779</td>\n",
       "      <td>29.556525</td>\n",
       "      <td>29.957940</td>\n",
       "      <td>30.452906</td>\n",
       "      <td>0.494965</td>\n",
       "      <td>1.086420</td>\n",
       "      <td>31.729366</td>\n",
       "      <td>27.383684</td>\n",
       "      <td>...</td>\n",
       "      <td>20.513784</td>\n",
       "      <td>22.800129</td>\n",
       "      <td>26.984316</td>\n",
       "      <td>6722.310059</td>\n",
       "      <td>2027.729980</td>\n",
       "      <td>55.167638</td>\n",
       "      <td>49.748691</td>\n",
       "      <td>47.086936</td>\n",
       "      <td>31.364504</td>\n",
       "      <td>30.839695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.839695</td>\n",
       "      <td>12640452</td>\n",
       "      <td>30.973283</td>\n",
       "      <td>29.664667</td>\n",
       "      <td>30.034423</td>\n",
       "      <td>30.513340</td>\n",
       "      <td>0.478916</td>\n",
       "      <td>1.094567</td>\n",
       "      <td>31.853801</td>\n",
       "      <td>27.475534</td>\n",
       "      <td>...</td>\n",
       "      <td>20.285738</td>\n",
       "      <td>22.459227</td>\n",
       "      <td>26.857002</td>\n",
       "      <td>6709.040039</td>\n",
       "      <td>2028.770020</td>\n",
       "      <td>55.071723</td>\n",
       "      <td>49.553518</td>\n",
       "      <td>46.803017</td>\n",
       "      <td>30.839695</td>\n",
       "      <td>30.944656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>78.301529</td>\n",
       "      <td>13675457</td>\n",
       "      <td>75.931026</td>\n",
       "      <td>74.702836</td>\n",
       "      <td>75.305196</td>\n",
       "      <td>75.725497</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>1.705192</td>\n",
       "      <td>78.113220</td>\n",
       "      <td>71.292452</td>\n",
       "      <td>...</td>\n",
       "      <td>38.856968</td>\n",
       "      <td>148.141144</td>\n",
       "      <td>82.393753</td>\n",
       "      <td>12302.190430</td>\n",
       "      <td>9682.910156</td>\n",
       "      <td>57.910147</td>\n",
       "      <td>55.622779</td>\n",
       "      <td>55.790021</td>\n",
       "      <td>76.887436</td>\n",
       "      <td>77.814888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>77.814888</td>\n",
       "      <td>9292930</td>\n",
       "      <td>76.515814</td>\n",
       "      <td>74.863687</td>\n",
       "      <td>75.491099</td>\n",
       "      <td>76.046941</td>\n",
       "      <td>0.555842</td>\n",
       "      <td>1.816324</td>\n",
       "      <td>78.496336</td>\n",
       "      <td>71.231038</td>\n",
       "      <td>...</td>\n",
       "      <td>38.423382</td>\n",
       "      <td>146.373550</td>\n",
       "      <td>81.836136</td>\n",
       "      <td>12286.980469</td>\n",
       "      <td>9615.809570</td>\n",
       "      <td>57.817099</td>\n",
       "      <td>55.415062</td>\n",
       "      <td>55.480641</td>\n",
       "      <td>78.191500</td>\n",
       "      <td>78.492363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>78.492363</td>\n",
       "      <td>10413347</td>\n",
       "      <td>77.157852</td>\n",
       "      <td>75.097238</td>\n",
       "      <td>75.713415</td>\n",
       "      <td>76.423160</td>\n",
       "      <td>0.709745</td>\n",
       "      <td>1.857931</td>\n",
       "      <td>78.813100</td>\n",
       "      <td>71.381377</td>\n",
       "      <td>...</td>\n",
       "      <td>38.469505</td>\n",
       "      <td>144.916718</td>\n",
       "      <td>82.576424</td>\n",
       "      <td>12641.440430</td>\n",
       "      <td>9814.080078</td>\n",
       "      <td>57.723926</td>\n",
       "      <td>55.207594</td>\n",
       "      <td>55.171523</td>\n",
       "      <td>77.972240</td>\n",
       "      <td>79.103050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>79.103050</td>\n",
       "      <td>9956838</td>\n",
       "      <td>77.684024</td>\n",
       "      <td>75.428935</td>\n",
       "      <td>75.964499</td>\n",
       "      <td>76.835451</td>\n",
       "      <td>0.870952</td>\n",
       "      <td>1.960788</td>\n",
       "      <td>79.350511</td>\n",
       "      <td>71.507360</td>\n",
       "      <td>...</td>\n",
       "      <td>39.087601</td>\n",
       "      <td>144.916718</td>\n",
       "      <td>82.364914</td>\n",
       "      <td>12836.599609</td>\n",
       "      <td>9924.750000</td>\n",
       "      <td>57.630627</td>\n",
       "      <td>55.000389</td>\n",
       "      <td>54.862711</td>\n",
       "      <td>78.309688</td>\n",
       "      <td>78.492363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>78.492363</td>\n",
       "      <td>8846168</td>\n",
       "      <td>77.893947</td>\n",
       "      <td>75.695202</td>\n",
       "      <td>76.151748</td>\n",
       "      <td>77.090360</td>\n",
       "      <td>0.938612</td>\n",
       "      <td>2.043482</td>\n",
       "      <td>79.782166</td>\n",
       "      <td>71.608238</td>\n",
       "      <td>...</td>\n",
       "      <td>38.635563</td>\n",
       "      <td>144.839020</td>\n",
       "      <td>85.018433</td>\n",
       "      <td>12619.519531</td>\n",
       "      <td>9953.750000</td>\n",
       "      <td>57.537207</td>\n",
       "      <td>54.793461</td>\n",
       "      <td>54.554253</td>\n",
       "      <td>79.039159</td>\n",
       "      <td>78.024811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2717 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Close    Volume        ma7       ma21      26ema      12ema  \\\n",
       "0     31.011450  14227229  30.050436  29.200291  29.514006  29.867322   \n",
       "1     31.440840  20081566  30.318975  29.299346  29.688906  30.115697   \n",
       "2     31.183207  10438080  30.564340  29.420211  29.822316  30.283528   \n",
       "3     31.364504  10302154  30.858779  29.556525  29.957940  30.452906   \n",
       "4     30.839695  12640452  30.973283  29.664667  30.034423  30.513340   \n",
       "...         ...       ...        ...        ...        ...        ...   \n",
       "2712  78.301529  13675457  75.931026  74.702836  75.305196  75.725497   \n",
       "2713  77.814888   9292930  76.515814  74.863687  75.491099  76.046941   \n",
       "2714  78.492363  10413347  77.157852  75.097238  75.713415  76.423160   \n",
       "2715  79.103050   9956838  77.684024  75.428935  75.964499  76.835451   \n",
       "2716  78.492363   8846168  77.893947  75.695202  76.151748  77.090360   \n",
       "\n",
       "          MACD      20sd  upper_band  lower_band  ...        GSK         LLY  \\\n",
       "0     0.353316  0.835852   30.871996   27.528586  ...  20.441219   22.546120   \n",
       "1     0.426791  0.963549   31.226444   27.372247  ...  20.290920   22.639698   \n",
       "2     0.461212  1.026446   31.473102   27.367319  ...  20.280550   22.492653   \n",
       "3     0.494965  1.086420   31.729366   27.383684  ...  20.513784   22.800129   \n",
       "4     0.478916  1.094567   31.853801   27.475534  ...  20.285738   22.459227   \n",
       "...        ...       ...         ...         ...  ...        ...         ...   \n",
       "2712  0.420300  1.705192   78.113220   71.292452  ...  38.856968  148.141144   \n",
       "2713  0.555842  1.816324   78.496336   71.231038  ...  38.423382  146.373550   \n",
       "2714  0.709745  1.857931   78.813100   71.381377  ...  38.469505  144.916718   \n",
       "2715  0.870952  1.960788   79.350511   71.507360  ...  39.087601  144.916718   \n",
       "2716  0.938612  2.043482   79.782166   71.608238  ...  38.635563  144.839020   \n",
       "\n",
       "            NVS          NYSE       NASDAQ        FT3        FT6        FT9  \\\n",
       "0     26.214659   6671.140137  2017.979980  55.454947  50.337802  47.947327   \n",
       "1     26.532934   6697.220215  2024.229980  55.359252  50.140845  47.659130   \n",
       "2     26.608170   6687.939941  2024.430054  55.263482  49.944470  47.372320   \n",
       "3     26.984316   6722.310059  2027.729980  55.167638  49.748691  47.086936   \n",
       "4     26.857002   6709.040039  2028.770020  55.071723  49.553518  46.803017   \n",
       "...         ...           ...          ...        ...        ...        ...   \n",
       "2712  82.393753  12302.190430  9682.910156  57.910147  55.622779  55.790021   \n",
       "2713  81.836136  12286.980469  9615.809570  57.817099  55.415062  55.480641   \n",
       "2714  82.576424  12641.440430  9814.080078  57.723926  55.207594  55.171523   \n",
       "2715  82.364914  12836.599609  9924.750000  57.630627  55.000389  54.862711   \n",
       "2716  85.018433  12619.519531  9953.750000  57.537207  54.793461  54.554253   \n",
       "\n",
       "          ARIMA  Close_lead1  \n",
       "0     31.011450    31.440840  \n",
       "1     31.440840    31.183207  \n",
       "2     31.183207    31.364504  \n",
       "3     31.364504    30.839695  \n",
       "4     30.839695    30.944656  \n",
       "...         ...          ...  \n",
       "2712  76.887436    77.814888  \n",
       "2713  78.191500    78.492363  \n",
       "2714  77.972240    79.103050  \n",
       "2715  78.309688    78.492363  \n",
       "2716  79.039159    78.024811  \n",
       "\n",
       "[2717 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_MRK_prediction.csv')\n",
    "df = df.drop(['Date' ,'Unnamed: 0'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa9b52",
   "metadata": {},
   "source": [
    "We identify the dependent and independent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53abe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Close_lead1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cedb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12ema',\n",
       " '20sd',\n",
       " '26ema',\n",
       " 'AMGN',\n",
       " 'ARIMA',\n",
       " 'BMY',\n",
       " 'FT3',\n",
       " 'FT6',\n",
       " 'FT9',\n",
       " 'GSK',\n",
       " 'JNJ',\n",
       " 'LLY',\n",
       " 'MACD',\n",
       " 'NASDAQ',\n",
       " 'NVS',\n",
       " 'NYSE',\n",
       " 'PFE',\n",
       " 'SNP',\n",
       " 'SNY',\n",
       " 'VTRS',\n",
       " 'VZ',\n",
       " 'Volume',\n",
       " 'ema',\n",
       " 'log_momentum',\n",
       " 'lower_band',\n",
       " 'ma21',\n",
       " 'ma7',\n",
       " 'momentum',\n",
       " 'neg',\n",
       " 'neu',\n",
       " 'pos',\n",
       " 'upper_band']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(df.columns.difference([\"Close\", 'Close_lead1']))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65613d0",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7baf43",
   "metadata": {},
   "source": [
    "To process the data, we first split it into training and test data, where two-thirds of the data is used for training, and the last third is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "465d24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(df) * 0.67)\n",
    "\n",
    "df_train = df.loc[:size].copy()\n",
    "df_test = df.loc[size:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc621071",
   "metadata": {},
   "source": [
    "Next, in order to ensure that some values don't inherently dominate the features due to their magnitude, we standardize their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff7d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = df_train[target].mean()\n",
    "target_stdev = df_train[target].std()\n",
    "\n",
    "for c in df_train.columns:\n",
    "    mean = df_train[c].mean()\n",
    "    stdev = df_train[c].std()\n",
    "\n",
    "    df_train[c] = (df_train[c] - mean) / stdev\n",
    "    df_test[c] = (df_test[c] - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134bb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[self.target].values).float()\n",
    "        self.X = torch.tensor(dataframe[self.features].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b59fe",
   "metadata": {},
   "source": [
    "Finally, the last step in the data processing to prepare it for LSTM is to prepare the data in a sequence of past observations. Preparing LSTM on time series data means that it uses a certain number of past observations to predict the future. In this case, the sequence length decides how many days the LSTM considers in advance. If the sequence length is $n$, then the LSTM considers the last $n$ observations to predict the $n+1$th day.\n",
    "\n",
    "The sequence length we decided on is 3 for purposes of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3029e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([1, 30, 32])\n",
      "Target shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "#batch_size = 10\n",
    "#sequence_length = 3\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa7dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for _ in train_loader:\n",
    "    i += 1 \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245af1d7",
   "metadata": {},
   "source": [
    "# Classical LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a784bc",
   "metadata": {},
   "source": [
    "We first define two functions:\n",
    "    \n",
    "- train_model: function to train the model based on the batches of data\n",
    "- test_model: function to test the model on the testing data\n",
    "    \n",
    "For both, they print the loss at the end to let us know how the model is performing with regards to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbe496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5d66a",
   "metadata": {},
   "source": [
    "## Running the Classical LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7aabb",
   "metadata": {},
   "source": [
    "To understand how we implemented QLSTM, we have to first understand how we implemented LSTM. LSTM follows the following structure:\n",
    "\n",
    "<img src=\"lstm2.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Image taken from: Quantum Long Short-Term Memory, https://arxiv.org/pdf/2009.01783.pdf. By Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang\n",
    "\n",
    "Simply put, LSTM uses 4 neural network layers in each LSTM cell to perform its functions. They are:\n",
    "\n",
    "- Forget layer\n",
    "- Input layer\n",
    "- Update layer\n",
    "- Output layer\n",
    "\n",
    "You can see the corresponding layers in the W cells in the picture above. We will be skipping the technical details, as you can find better information on the theory elsewhere, but it is important to note that these 4 layers are the key to building an LSTM neural network model that we can train and eventually use to predict, and usually take the form of a normal NN layer (like a linear layer with reLU or convolutional layers).\n",
    "\n",
    "LSTMs are well studied, and there is a native implementation of it in PyTorch to begin with, so we use a slightly modified version of it for the time series LSTM that we perform here. The code for the time series LSTM was reused from:\n",
    "\n",
    "How to use PyTorch LSTMs for time series regression: https://www.crosstab.io/articles/time-series-pytorch-lstm, Brian Kent.\n",
    "\n",
    "In the following code, we train LSTM to predict future stock prices, and then test it on the test dataset. The learning rate of 0.0001 was decided after some experimentation, where we chose the learning rate that most reliably gave accurate results. The number of epochs we use is 20, by which it would have converged and thus would suffice for the purposes of this notebook. After that, we proceed to see 3 different graphs: the comparison between the real stock prices and the ones given by the model; and the evolution of test loss and training loss by epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fc107",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b0568ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, n_qubits=0, n_qlayers=1\n",
    "                 #, num_layers, dropout_value\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_value = dropout_value\n",
    "\n",
    "        self.lstm = QLSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            n_qubits = n_qubits,\n",
    "            n_qlayers= n_qlayers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a758f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_shapes = (n_qlayers, n_vrotations, n_qubits) = (1, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "#learning_rate = 0.0001\n",
    "#num_hidden_units = 16\n",
    "\n",
    "model = ShallowRegressionLSTM(num_sensors=len(features), hidden_units=num_hidden_units, n_qubits=4)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= weight_decay_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2cdd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "--------\n",
      "Test loss: 6.597873733195174\n",
      "Execution time 1302.7725920677185\n",
      "Epoch 0\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\Anaconda3\\envs\\qrnn\\lib\\site-packages\\torch\\autograd\\__init__.py:173: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14816461981785436\n",
      "Test loss: 2.7046614232575106\n",
      "Execution time 5318.334117650986\n",
      "Epoch 1\n",
      "---------\n",
      "Train loss: 0.13623541970835903\n",
      "Test loss: 2.7175407673535235\n",
      "Execution time 5319.161185264587\n",
      "Epoch 2\n",
      "---------\n",
      "Train loss: 0.14224370016102994\n",
      "Test loss: 3.5212133973020077\n",
      "Execution time 5315.983252048492\n",
      "Epoch 3\n",
      "---------\n",
      "Train loss: 0.09548015852759979\n",
      "Test loss: 2.594655237563075\n",
      "Execution time 5398.879703998566\n",
      "Epoch 4\n",
      "---------\n",
      "Train loss: 0.09206215778050478\n",
      "Test loss: 3.008570600040512\n",
      "Execution time 5489.611770868301\n",
      "Epoch 5\n",
      "---------\n",
      "Train loss: 0.10468765771423166\n",
      "Test loss: 2.78595284589459\n",
      "Execution time 5503.001051664352\n",
      "Epoch 6\n",
      "---------\n",
      "Train loss: 0.09625351373622787\n",
      "Test loss: 3.03025396478397\n",
      "Execution time 5326.096094608307\n",
      "Epoch 7\n",
      "---------\n",
      "Train loss: 0.10390485516926543\n",
      "Test loss: 2.687387861979946\n",
      "Execution time 5325.742523431778\n",
      "Epoch 8\n",
      "---------\n",
      "Train loss: 0.09396903098013216\n",
      "Test loss: 3.2179606967904673\n",
      "Execution time 5308.123530864716\n",
      "Epoch 9\n",
      "---------\n",
      "Train loss: 0.20628454260874107\n",
      "Test loss: 3.931796972621689\n",
      "Execution time 5315.727384567261\n",
      "Epoch 10\n",
      "---------\n",
      "Train loss: 0.15663198089446423\n",
      "Test loss: 3.8834286649865812\n",
      "Execution time 5316.313944101334\n",
      "Epoch 11\n",
      "---------\n",
      "Train loss: 0.12023876828202772\n",
      "Test loss: 3.3332360217798387\n",
      "Execution time 5309.277529478073\n",
      "Epoch 12\n",
      "---------\n",
      "Train loss: 0.12677361789212435\n",
      "Test loss: 3.854500485845782\n",
      "Execution time 5320.5580701828\n",
      "Epoch 13\n",
      "---------\n",
      "Train loss: 0.11292700443695171\n",
      "Test loss: 3.235656696927199\n",
      "Execution time 5353.90164732933\n",
      "Epoch 14\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "classical_loss_train = []\n",
    "classical_loss_test = []\n",
    "classical_time = []\n",
    "print(\"Untrained test\\n--------\")\n",
    "start = time.time()\n",
    "test_loss = test_model(test_loader, model, loss_function)\n",
    "end = time.time()\n",
    "print(\"Execution time\", end - start)\n",
    "classical_loss_test.append(test_loss)\n",
    "\n",
    "for ix_epoch in range(num_epochs):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    start = time.time()\n",
    "    train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    test_loss = test_model(test_loader, model, loss_function)\n",
    "    end = time.time()\n",
    "    print(\"Execution time\", end - start)\n",
    "    classical_loss_train.append(train_loss)\n",
    "    classical_loss_test.append(test_loss)\n",
    "    classical_time.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44945e",
   "metadata": {},
   "source": [
    "We then use the model to predict the test set, and then compare the results of the prediction to the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ffa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n",
    "    function.\n",
    "    \"\"\"\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_star = model(X)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58531cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ystar_col = \"Model forecast\"\n",
    "df_train[ystar_col] = predict(train_eval_loader, model).numpy()\n",
    "df_test[ystar_col] = predict(test_loader, model).numpy()\n",
    "\n",
    "df_out = pd.concat((df_train, df_test))[[target, ystar_col]]\n",
    "\n",
    "for c in df_out.columns:\n",
    "    df_out[c] = df_out[c] * target_stdev + target_mean\n",
    "\n",
    "print(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out[~df_out.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ed8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(range(len(df_out)), df_out[\"Close_lead1\"], label = \"Real\")\n",
    "plt.plot(range(len(df_out)), df_out[\"Model forecast\"], label = \"LSTM Prediction\")\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Days')\n",
    "plt.vlines(size, ymin = 30, ymax = 90, label = \"Test set start\", linestyles = \"dashed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(21), classical_loss_test)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 21), classical_loss_train)\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time = sum(classical_time)/num_epochs\n",
    "avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515841ce",
   "metadata": {},
   "source": [
    "## Number of parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1eebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_par = count_parameters(model)\n",
    "num_par"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e3f02",
   "metadata": {},
   "source": [
    "# Trading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_model(data_loader, model, loss_function, initial_inv, initial_price):\n",
    "    \n",
    "    # initial seed money\n",
    "    inv = initial_inv\n",
    "    \n",
    "    # initial price of stock\n",
    "    price = initial_price\n",
    "    \n",
    "    # initial number of stock \n",
    "    number_stocks = 0\n",
    "    \n",
    "    # predicted earnings\n",
    "    pred_earn = 0 \n",
    "    \n",
    "    # predicted stock\n",
    "    pred_output = []\n",
    "    \n",
    "    # day\n",
    "    day = 1\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(X)\n",
    "            \n",
    "            pred_price =  output * target_stdev + target_mean\n",
    "            pred_output.append(pred_price)\n",
    "            print(\"Today's stock price: \", price)\n",
    "            print(\"Predicted stock price for tomorrow: \", pred_price)\n",
    "            \n",
    "            #if predicted value next day higher than today's price\n",
    "            if pred_price > price:\n",
    "                number_stocks = number_stocks + inv/price\n",
    "                inv = 0\n",
    "                print(\"Day\", day, \": Bought/Keep \", number_stocks, \"stocks at price \", price)\n",
    "                pred_earn = pred_earn + number_stocks * (pred_price - price)\n",
    "                print(\"Predicted earnings: \", pred_earn)\n",
    "                \n",
    "                \n",
    "            #if predicted value next day lower than (or equal to)  today's price\n",
    "            else:\n",
    "                inv = inv + number_stocks * price\n",
    "                print(\"Day\", day, \": Sold \", number_stocks, \"at price \", price)\n",
    "                number_stocks = 0\n",
    "            \n",
    "            # Update price\n",
    "            price = y * target_stdev + target_mean\n",
    "            day += 1\n",
    "            print(\"Current number of stocks: \", number_stocks)\n",
    "            print(\"Current seed money:\", inv)\n",
    "        \n",
    "        model.train()\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    total_earnings = number_stocks*price + inv - initial_inv\n",
    "    print(f\"Trading earnings: {total_earnings}\")\n",
    "    print(f\"Predicted earnings: {pred_earn}\")\n",
    "    return total_earnings, pred_earn, pred_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_inv = 10000\n",
    "initial_price = df_out[\"Close_lead1\"][size - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a468b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_earnings, pred_earnings, pred_output = trading_model(test_loader, model, loss_function, initial_inv, initial_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(range(len(df_out)), df_out[\"Close_lead1\"], label = \"Real\")\n",
    "plt.plot(range(size, len(df_out)), pred_output, label = \"LSTM Trading Strategy Prediction\")\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Days')\n",
    "plt.vlines(size, ymin = 30, ymax = 90, label = \"Test set start\", linestyles = \"dashed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27028bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_earnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932707ba",
   "metadata": {},
   "source": [
    "# Store Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6feab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce89987",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.csv', 'a', newline='') as test_csv:\n",
    "    test_csv_writer = csv.writer(test_csv)\n",
    "    test_csv_writer.writerow(classical_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.csv', 'a', newline='') as train_csv:\n",
    "    train_csv_writer = csv.writer(train_csv)\n",
    "    train_csv_writer.writerow(classical_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pred.csv', 'a', newline='') as pred_csv:\n",
    "    pred_csv_writer = csv.writer(pred_csv)\n",
    "    pred_csv_writer.writerow(df_out[\"Model forecast\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34db007",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('time.csv', 'a', newline='') as time_csv:\n",
    "    time_csv_writer = csv.writer(time_csv)\n",
    "    time_csv_writer.writerow([avg_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572915e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameters.csv', 'a', newline='') as parameters_csv:\n",
    "    parameters_csv_writer = csv.writer(parameters_csv)\n",
    "    parameters_csv_writer.writerow([num_par])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad26287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_earnings.csv', 'a', newline='') as total_earnings_csv:\n",
    "    total_earnings_csv_writer = csv.writer(total_earnings_csv)\n",
    "    total_earnings_csv_writer.writerow([total_earnings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68509d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pred_earnings.csv', 'a', newline='') as pred_earnings_csv:\n",
    "    pred_earnings_csv_writer = csv.writer(pred_earnings_csv)\n",
    "    pred_earnings_csv_writer.writerow([pred_earnings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pred_output.csv', 'a', newline='') as pred_output_csv:\n",
    "    pred_output_csv_writer = csv.writer(pred_output_csv)\n",
    "    pred_output_csv_writer.writerow(pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classical_loss_test)):\n",
    "    writer.add_scalar(\"Test loss\",classical_loss_test[i], i)\n",
    "    \n",
    "for i in range(len(classical_loss_train)):\n",
    "    writer.add_scalar(\"Train loss: \", classical_loss_train[i], i)\n",
    "    \n",
    "for i in range(len(df_out[\"Model forecast\"])):\n",
    "    writer.add_scalar(\"Predicted Value: \", df_out[\"Model forecast\"][i], i)\n",
    "    \n",
    "for i in range(len(dpred_output)):\n",
    "    writer.add_scalar(\"Predicted Value (Dynamic): \", pred_output, size + i)\n",
    "    \n",
    "writer.add_scalar(\"Avg Time: \", avg_time, 1)\n",
    "writer.add_scalar(\"Number of parameters: \", num_par, 1)\n",
    "writer.add_scalar(\"Total earnings: \", total_earnings, 1)\n",
    "writer.add_scalar(\"Predicted earnings: \", pred_earnings, 1)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a1f21",
   "metadata": {},
   "source": [
    "## Complexity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60edef",
   "metadata": {},
   "source": [
    "As a last point of comparison, we want to discuss the complexity analysis for the QLSTM, as well as compare the number of parameters used for the LSTM and the QLSTM.\n",
    "\n",
    "Given that we would want to eventually use the QLSTM on real quantum computers, it would be prudent to discuss its viability in terms of the number of gates and qubits used. First of all, we look at the number of qubits. As can be seen, in a barebones early version of QLSTM, we can afford to use any number of qubits, even a small number like 4 to run it. This is because we can sandwich it between two classical layers, where the classical layers help to convert the vector sizes to the correct amount, from the embedding of input features into the quantum circuit to the processing of the output measurements. This is very similar in idea to dressed quantum circuits, which was presented in:\n",
    "\n",
    "Transfer learning in hybrid classical-quantum neural networks: https://arxiv.org/abs/1912.08278, Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, Nathan Killoran\n",
    "\n",
    "In the foreseeable future in the NISQ era, we would likely be continuing to use the same technique, although the number of qubits can be increased to improve the model, making this technique useful in the NISQ era.\n",
    "\n",
    "For the number of gates used per circuit, we take a look at how the overall depth would increase as number of qubits and number of layers increases. As seen from the circuit above, the overall depth is $3 + n_l * (2 * n_q + 3)$, where $n_l, n_q$ refers to the number of layers and number of qubits respectively. Thus, the overall depth would increase linearly in both number of qubits and number of layers. What this also means is that the depth is quite easy to control, and extremely viable for the NISQ era. In this particular proof of example, we show that even when the number of layers is 1 (the smallest it can be) and the number of qubits is a small number, we still get pretty good results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f526c77",
   "metadata": {},
   "source": [
    "# Conclusion and Future Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198269c2",
   "metadata": {},
   "source": [
    "In this notebook, we have given the proof of concept that QLSTM can be used to great effect for stock price prediction, offering comparable results than its classical counterpart, while needed much fewer parameters to train and getting more information per epoch. All in all, this shows that this technique has a lot of potential in the financial world and more, as anything time series related can be trained using an LSTM and hence also a QLSTM. This is relevant not only to stock price predictions, but also for example company sales predictions or other key performance predictions. However, there are still many areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
