{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf777cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec491972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=6,\n",
    "                n_qlayers=1,\n",
    "                n_vrotations=3,\n",
    "                batch_first=False,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"\n",
    "                ):\n",
    "        super(QGRU, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        #gates\n",
    "        \n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_reset = [f\"wire_reset_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_memory = [f\"wire_memory_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_reset = qml.device(self.backend, wires=self.wires_reset)\n",
    "        self.dev_memory = qml.device(self.backend, wires=self.wires_memory)\n",
    "        \n",
    "        def ansatz(params, wires_type):\n",
    "            # Entangling layer.\n",
    "            for i in range(1,3): \n",
    "                for j in range(self.n_qubits):\n",
    "                    if j + i < self.n_qubits:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i]])\n",
    "                    else:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i - self.n_qubits]])\n",
    "\n",
    "            # Variational layer.\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[0][i], wires=wires_type[i])\n",
    "                qml.RY(params[1][i], wires=wires_type[i])\n",
    "                qml.RZ(params[2][i], wires=wires_type[i])\n",
    "                \n",
    "        def VQC(features, weights, wires_type):\n",
    "            # Preproccess input data to encode the initial state.\n",
    "            #qml.templates.AngleEmbedding(features, wires=wires_type)\n",
    "            ry_params = [torch.arctan(feature) for feature in features]\n",
    "            rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=wires_type[i])\n",
    "                qml.RY(ry_params[i], wires=wires_type[i])\n",
    "                qml.RZ(rz_params[i], wires=wires_type[i])\n",
    "        \n",
    "            #Variational block.\n",
    "            qml.layer(ansatz, self.n_qlayers, weights, wires_type = wires_type)\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\", diff_method='adjoint')\n",
    "\n",
    "        def _circuit_reset(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_reset)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_reset]\n",
    "        self.qlayer_reset = qml.QNode(_circuit_reset, self.dev_reset, interface=\"torch\", diff_method='adjoint')\n",
    "        \n",
    "        def _circuit_memory(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_memory)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_memory]\n",
    "        self.qlayer_memory = qml.QNode(_circuit_memory, self.dev_memory, interface=\"torch\", diff_method='adjoint')\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations, self.n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations, n_qubits) = ({self.n_qlayers}, {self.n_vrotations}, {self.n_qubits})\")\n",
    "\n",
    "        self.h2h = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.clayer_in2 = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.VQC = {\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'reset': qml.qnn.TorchLayer(self.qlayer_reset, weight_shapes),\n",
    "            'memory': qml.qnn.TorchLayer(self.qlayer_memory, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        self.clayer_out2 = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        self.clayer_out3 = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state \n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t = init_states\n",
    "            h_t = h_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            if self.batch_first is True:\n",
    "                x_t = x[:, t , :]\n",
    "            else:\n",
    "                x_t = x[t, : , :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            z_t = torch.sigmoid(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            r_t = torch.sigmoid(self.clayer_out2(self.VQC['reset'](y_t)))  # reset block\n",
    "            \n",
    "            h_t_new = self.h2h(h_t)\n",
    "            \n",
    "            g_t = (r_t * h_t)\n",
    "            \n",
    "            v_prime_t = torch.cat((g_t, x_t), dim=1)\n",
    "            \n",
    "            y_prime_t = self.clayer_in2(v_prime_t)\n",
    "            \n",
    "            h_prime_t = torch.tanh(self.clayer_out3(self.VQC['memory'](y_prime_t))) # memory\n",
    "            \n",
    "            o1_t = (z_t * h_t)\n",
    "            o2_t = ((-1)*(z_t - 1)) * h_prime_t\n",
    "            \n",
    "            h_t = o1_t + o2_t\n",
    "\n",
    "            h_t = h_t.unsqueeze(0)\n",
    "            hidden_seq.append(h_t)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        #hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, h_t\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a55322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4,\n",
    "                n_qlayers=1,\n",
    "                n_vrotations=3,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        #self.dev_forget = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_input = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_update = qml.device(self.backend, wires=self.n_qubits)\n",
    "        #self.dev_output = qml.device(self.backend, wires=self.n_qubits)\n",
    "        \n",
    "        def ansatz(params, wires_type):\n",
    "            # Entangling layer.\n",
    "            for i in range(1,3): \n",
    "                for j in range(self.n_qubits):\n",
    "                    if j + i < self.n_qubits:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i]])\n",
    "                    else:\n",
    "                        qml.CNOT(wires=[wires_type[j], wires_type[j + i - self.n_qubits]])\n",
    "\n",
    "            # Variational layer.\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[0][i], wires=wires_type[i])\n",
    "                qml.RY(params[1][i], wires=wires_type[i])\n",
    "                qml.RZ(params[2][i], wires=wires_type[i])\n",
    "                \n",
    "        def VQC(features, weights, wires_type):\n",
    "            # Preproccess input data to encode the initial state.\n",
    "            #qml.templates.AngleEmbedding(features, wires=wires_type)\n",
    "            ry_params = [torch.arctan(feature) for feature in features]\n",
    "            rz_params = [torch.arctan(feature**2) for feature in features]\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=wires_type[i])\n",
    "                qml.RY(ry_params[i], wires=wires_type[i])\n",
    "                qml.RZ(ry_params[i], wires=wires_type[i])\n",
    "        \n",
    "            #Variational block.\n",
    "            qml.layer(ansatz, self.n_qlayers, weights, wires_type = wires_type)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations, self.n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations, n_qubits) = ({self.n_qlayers}, {self.n_vrotations}, {self.n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, self.n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "        #self.clayer_out = [torch.nn.Linear(n_qubits, self.hidden_size) for _ in range(4)]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            if self.batch_first is True:\n",
    "                x_t = x[:, t , :]\n",
    "            else:\n",
    "                x_t = x[t, : , :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackAugmentedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, layer_type='GRU',\n",
    "                 n_layers=1, is_bidirectional=False, has_stack=False,\n",
    "                 stack_width=None, stack_depth=None, ignore_idx=0,\n",
    "                 use_cuda=None, optimizer_instance=torch.optim.Adadelta,\n",
    "                 lr=0.01, backend = \"default.qubit\", n_qubits = 8):\n",
    "        \"\"\"\n",
    "        Constructor for the StackAugmentedRNN object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: int\n",
    "            number of characters in the alphabet\n",
    "\n",
    "        hidden_size: int\n",
    "            size of the RNN layer(s)\n",
    "\n",
    "        output_size: int\n",
    "            again number of characters in the alphabet\n",
    "\n",
    "        layer_type: str (default 'GRU')\n",
    "            type of the RNN layer to be used. Could be either 'LSTM' or 'GRU'.\n",
    "\n",
    "        n_layers: int (default 1)\n",
    "            number of RNN layers\n",
    "\n",
    "        is_bidirectional: bool (default False)\n",
    "            parameter specifying if RNN is bidirectional\n",
    "\n",
    "        has_stack: bool (default False)\n",
    "            parameter specifying if augmented memory stack is used\n",
    "\n",
    "        stack_width: int (default None)\n",
    "            if has_stack is True then this parameter defines width of the\n",
    "            augmented stack memory\n",
    "\n",
    "        stack_depth: int (default None)\n",
    "            if has_stack is True then this parameter define depth of the augmented\n",
    "            stack memory. Hint: no need fo stack depth to be larger than the\n",
    "            length of the longest sequence you plan to generate\n",
    "\n",
    "        use_cuda: bool (default None)\n",
    "            parameter specifying if GPU is used for computations. If left\n",
    "            unspecified, GPU will be used if available\n",
    "\n",
    "        optimizer_instance: torch.optim object (default torch.optim.Adadelta)\n",
    "            optimizer to be used for training\n",
    "\n",
    "        lr: float (default 0.01)\n",
    "            learning rate for the optimizer\n",
    "\n",
    "        \"\"\"\n",
    "        super(StackAugmentedRNN, self).__init__()\n",
    "\n",
    "        if layer_type not in ['GRU', 'LSTM', 'QGRU', 'QLSTM']:\n",
    "            raise InvalidArgumentError('Layer type must be GRU (QGRU) or LSTM (QLSTM)')\n",
    "        self.layer_type = layer_type\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "        if self.is_bidirectional:\n",
    "            self.num_dir = 2\n",
    "        else:\n",
    "            self.num_dir = 1\n",
    "        if layer_type == 'LSTM' or layer_type == 'QLSTM':\n",
    "            self.has_cell = True\n",
    "        else:\n",
    "            self.has_cell = False\n",
    "        self.has_stack = has_stack\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        if self.has_stack:\n",
    "            self.stack_width = stack_width\n",
    "            self.stack_depth = stack_depth\n",
    "        \n",
    "\n",
    "        self.use_cuda = use_cuda\n",
    "        if self.use_cuda is None:\n",
    "            self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        if self.has_stack:\n",
    "            self.stack_controls_layer = nn.Linear(in_features=self.hidden_size *\n",
    "                                                              self.num_dir,\n",
    "                                                  out_features=3)\n",
    "\n",
    "            self.stack_input_layer = nn.Linear(in_features=self.hidden_size *\n",
    "                                                           self.num_dir,\n",
    "                                               out_features=self.stack_width)\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.has_stack:\n",
    "            rnn_input_size = hidden_size + stack_width\n",
    "        else:\n",
    "            rnn_input_size = hidden_size\n",
    "        if self.layer_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(rnn_input_size, hidden_size, n_layers,\n",
    "                               bidirectional=self.is_bidirectional)\n",
    "            self.decoder = nn.Linear(hidden_size * self.num_dir, output_size)\n",
    "            \n",
    "        elif self.layer_type == 'QLSTM':\n",
    "            self.backend = backend\n",
    "            print(\"Backend: \", self.backend)\n",
    "            self.rnn = QLSTM(rnn_input_size, hidden_size, backend = self.backend, n_qubits = n_qubits)\n",
    "            self.decoder = nn.Linear(hidden_size * self.num_dir, output_size)\n",
    "        \n",
    "        elif self.layer_type == 'GRU':\n",
    "            self.rnn = nn.GRU(rnn_input_size, hidden_size, n_layers,\n",
    "                              bidirectional=self.is_bidirectional)\n",
    "            self.decoder = nn.Linear(hidden_size * self.num_dir, output_size)\n",
    "            \n",
    "        elif self.layer_type == 'QGRU':\n",
    "            self.backend = backend\n",
    "            print(\"Backend: \", self.backend)\n",
    "            self.rnn = QGRU(rnn_input_size, hidden_size, backend = self.backend, n_qubits = n_qubits)\n",
    "            self.decoder = nn.Linear(hidden_size * self.num_dir, output_size)\n",
    "            \n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.ignore_idx = ignore_idx\n",
    "        if self.use_cuda:\n",
    "            self = self.cuda()\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.ignore_idx)\n",
    "        self.lr = lr\n",
    "        self.optimizer_instance = optimizer_instance\n",
    "        self.optimizer = self.optimizer_instance(self.parameters(), lr=lr,\n",
    "                                                 weight_decay=0.00001)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Loads pretrained parameters from the checkpoint into the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            path to the checkpoint file model will be loaded from.\n",
    "        \"\"\"\n",
    "        weights = torch.load(path)\n",
    "        self.load_state_dict(weights)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saves model parameters into the checkpoint file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            path to the checkpoint file model will be saved to.\n",
    "        \"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def change_lr(self, new_lr):\n",
    "        \"\"\"\n",
    "        Updates learning rate of the optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_lr: float\n",
    "            new learning rate value\n",
    "        \"\"\"\n",
    "        self.optimizer = self.optimizer_instance(self.parameters(), lr=new_lr)\n",
    "        self.lr = new_lr\n",
    "\n",
    "    def forward(self, inp, hidden, stack):\n",
    "        \"\"\"\n",
    "        Forward step of the model. Generates probability of the next character\n",
    "        given the prefix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp: torch.tensor\n",
    "            input tensor that contains prefix string indices\n",
    "\n",
    "        hidden: torch.tensor or tuple(torch.tensor, torch.tensor)\n",
    "            previous hidden state of the model. If layer_type is 'LSTM',\n",
    "            then hidden is a tuple of hidden state and cell state, otherwise\n",
    "            hidden is torch.tensor\n",
    "\n",
    "        stack: torch.tensor\n",
    "            previous state of the augmented memory stack\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: torch.tensor\n",
    "            tensor with non-normalized probabilities of the next character\n",
    "\n",
    "        next_hidden: torch.tensor or tuple(torch.tensor, torch.tensor)\n",
    "            next hidden state of the model. If layer_type is 'LSTM',\n",
    "            then next_hidden is a tuple of hidden state and cell state,\n",
    "            otherwise next_hidden is torch.tensor\n",
    "\n",
    "        next_stack: torch.tensor\n",
    "            next state of the augmented memory stack\n",
    "        \"\"\"\n",
    "    \n",
    "        inp = self.encoder(inp)\n",
    "\n",
    "        if self.has_stack:\n",
    "            if self.has_cell:\n",
    "                hidden_ = hidden[0]\n",
    "            else:\n",
    "                hidden_ = hidden\n",
    "            if self.is_bidirectional:\n",
    "                hidden_2_stack = torch.cat((hidden_[0], hidden_[1]), dim=1)\n",
    "            else:\n",
    "                hidden_2_stack = hidden_.squeeze(0)\n",
    "            stack_controls = self.stack_controls_layer(hidden_2_stack)\n",
    "            stack_controls = F.softmax(stack_controls, dim=1)\n",
    "            stack_input = self.stack_input_layer(hidden_2_stack.unsqueeze(0))\n",
    "            stack_input = torch.tanh(stack_input)\n",
    "            stack = self.stack_augmentation(stack_input.permute(1, 0, 2),\n",
    "                                            stack, stack_controls)\n",
    "            stack_top = stack[:, 0, :].unsqueeze(0)\n",
    "            inp = torch.cat((inp.unsqueeze(0), stack_top), dim=2)\n",
    "        else:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        output, next_hidden = self.rnn(inp, hidden)\n",
    "        output = self.decoder(output.squeeze(0))\n",
    "        return output, next_hidden, stack\n",
    "\n",
    "    def stack_augmentation(self, input_val, prev_stack, controls):\n",
    "        \"\"\"\n",
    "        Augmentation of the tensor into the stack. For more details see\n",
    "        https://arxiv.org/abs/1503.01007\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_val: torch.tensor\n",
    "            tensor to be added to stack\n",
    "\n",
    "        prev_stack: torch.tensor\n",
    "            previous stack state\n",
    "\n",
    "        controls: torch.tensor\n",
    "            predicted probabilities for each operation in the stack, i.e\n",
    "            PUSH, POP and NO_OP. Again, see https://arxiv.org/abs/1503.01007\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_stack: torch.tensor\n",
    "            new stack state\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = prev_stack.size(0)\n",
    "\n",
    "\n",
    "        controls = controls.view(-1, 3, 1, 1)\n",
    "        zeros_at_the_bottom = torch.zeros(batch_size, 1, self.stack_width)\n",
    "        if self.use_cuda:\n",
    "            zeros_at_the_bottom = Variable(zeros_at_the_bottom.cuda())\n",
    "        else:\n",
    "            zeros_at_the_bottom = Variable(zeros_at_the_bottom)\n",
    "        a_push, a_pop, a_no_op = controls[:, 0], controls[:, 1], controls[:, 2]\n",
    "        stack_down = torch.cat((prev_stack[:, 1:], zeros_at_the_bottom), dim=1)\n",
    "        stack_up = torch.cat((input_val, prev_stack[:, :-1]), dim=1)\n",
    "        new_stack = a_no_op * prev_stack + a_push * stack_up + a_pop * stack_down\n",
    "        return new_stack\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialization of the hidden state of RNN.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden: torch.tensor\n",
    "            tensor filled with zeros of an appropriate size (taking into\n",
    "            account number of RNN layers and directions)\n",
    "        \"\"\"\n",
    "        if self.use_cuda:\n",
    "            return Variable(torch.zeros(self.n_layers * self.num_dir,\n",
    "                                        batch_size,\n",
    "                                        self.hidden_size).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.n_layers * self.num_dir,\n",
    "                                        batch_size,\n",
    "                                        self.hidden_size))\n",
    "\n",
    "    def init_cell(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialization of the cell state of LSTM. Only used when layers_type is\n",
    "        'LSTM'\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cell: torch.tensor\n",
    "            tensor filled with zeros of an appropriate size (taking into\n",
    "            account number of RNN layers and directions)\n",
    "        \"\"\"\n",
    "        if self.use_cuda:\n",
    "            return Variable(torch.zeros(self.n_layers * self.num_dir,\n",
    "                                        batch_size,\n",
    "                                        self.hidden_size).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.n_layers * self.num_dir,\n",
    "                                        batch_size,\n",
    "                                        self.hidden_size))\n",
    "\n",
    "    def init_stack(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialization of the stack state. Only used when has_stack is True\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stack: torch.tensor\n",
    "            tensor filled with zeros\n",
    "        \"\"\"\n",
    "        result = torch.zeros(batch_size, self.stack_depth, self.stack_width)\n",
    "        if self.use_cuda:\n",
    "            return Variable(result.cuda())\n",
    "        else:\n",
    "            return Variable(result)\n",
    "\n",
    "    def train_step(self, inp, target):\n",
    "        \"\"\"\n",
    "        One train step, i.e. forward-backward and parameters update, for\n",
    "        a single training example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp: torch.tensor\n",
    "            tokenized training string from position 0 to position (seq_len - 1)\n",
    "\n",
    "        target:\n",
    "            tokenized training string from position 1 to position seq_len\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            mean value of the loss function (averaged through the sequence\n",
    "            length)\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = inp.size()[0]\n",
    "        seq_len = inp.size()[1]\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        if self.has_cell:\n",
    "            cell = self.init_cell(batch_size)\n",
    "            hidden = (hidden, cell)\n",
    "        if self.has_stack:\n",
    "            stack = self.init_stack(batch_size)\n",
    "        else:\n",
    "            stack = None\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for c in range(seq_len):\n",
    "            output, hidden, stack = self(inp[:, c], hidden, stack)\n",
    "            loss += self.criterion(output, target[:, c])\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item() / batch_size\n",
    "\n",
    "    def evaluate(self, data, prime_str='<', end_token='>', predict_len=100,\n",
    "                 batch_size=8):\n",
    "        \"\"\"\n",
    "        Generates new string from the model distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: object of type GeneratorData\n",
    "            stores information about the generator data format such alphabet, etc\n",
    "\n",
    "        prime_str: str (default '<')\n",
    "            prime string that will be used as prefix. Deafult value is just the\n",
    "            START_TOKEN\n",
    "\n",
    "        end_token: str (default '>')\n",
    "            when end_token is sampled from the model distribution,\n",
    "            the generation of a new example is finished\n",
    "\n",
    "        predict_len: int (default 100)\n",
    "            maximum length of the string to be generated. If the end_token is\n",
    "            not sampled, the generation will be aborted when the length of the\n",
    "            generated sequence is equal to predict_len\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_sample: str\n",
    "            Newly generated sample from the model distribution.\n",
    "\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        if self.has_cell:\n",
    "            cell = self.init_cell(batch_size)\n",
    "            hidden = (hidden, cell)\n",
    "        if self.has_stack:\n",
    "            stack = self.init_stack(batch_size)\n",
    "        else:\n",
    "            stack = None\n",
    "        prime_input, _ = data.seq2tensor([prime_str] * batch_size,\n",
    "                                         tokens=data.all_characters,\n",
    "                                         flip=False)\n",
    "        prime_input = torch.tensor(prime_input).long()\n",
    "        if self.use_cuda:\n",
    "            prime_input = prime_input.cuda()\n",
    "        new_samples = [[prime_str] * batch_size]\n",
    "\n",
    "        # Use priming string to \"build up\" hidden state\n",
    "        for p in range(len(prime_str[0]) - 1):\n",
    "            _, hidden, stack = self.forward(prime_input[:, p], hidden, stack)\n",
    "        inp = prime_input[:, -1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, hidden, stack = self.forward(inp, hidden, stack)\n",
    "\n",
    "            # Sample from the network as a multinomial distribution\n",
    "            probs = torch.softmax(output, dim=1).detach()  # .cpu().numpy()\n",
    "            # top_i = self.sample_from_probs(probs)\n",
    "            top_i = torch.multinomial(probs, 1).cpu().numpy()\n",
    "\n",
    "            # Add predicted character to string and use as next input\n",
    "            predicted_char = (np.array(data.all_characters)[top_i].reshape(-1))\n",
    "            predicted_char = predicted_char.tolist()\n",
    "            new_samples.append(predicted_char)\n",
    "\n",
    "            # Prepare next input token for the generator\n",
    "            inp, _ = data.seq2tensor(predicted_char, tokens=data.all_characters)\n",
    "            inp = torch.tensor(inp.squeeze(1)).long()\n",
    "            if self.use_cuda:\n",
    "                inp = inp.cuda()\n",
    "\n",
    "        # Remove characters after end tokens\n",
    "        string_samples = []\n",
    "        new_samples = np.array(new_samples)\n",
    "        for i in range(batch_size):\n",
    "            sample = list(new_samples[:, i])\n",
    "            if end_token in sample:\n",
    "                end_token_idx = sample.index(end_token)\n",
    "                string_samples.append(''.join(sample[:end_token_idx+1]))\n",
    "        return string_samples\n",
    "\n",
    "    def fit(self, data, batch_size, n_iterations, all_losses=[],\n",
    "            print_every=100, plot_every=10):\n",
    "        \"\"\"\n",
    "        This methods fits the parameters of the model. Training is performed to\n",
    "        minimize the cross-entropy loss when predicting the next character\n",
    "        given the prefix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: object of type GeneratorData\n",
    "            stores information about the generator data format such alphabet, etc\n",
    "\n",
    "        n_iterations: int\n",
    "            how many iterations of training will be performed\n",
    "\n",
    "        all_losses: list (default [])\n",
    "            list to store the values of the loss function\n",
    "\n",
    "        print_every: int (default 100)\n",
    "            feedback will be printed to std_out once every print_every\n",
    "            iterations of training\n",
    "\n",
    "        plot_every: int (default 10)\n",
    "            value of the loss function will be appended to all_losses once every\n",
    "            plot_every iterations of training\n",
    "\n",
    "        augment: bool (default False)\n",
    "            parameter specifying if SMILES enumeration will be used. For mode\n",
    "            details on SMILES enumeration see https://arxiv.org/abs/1703.07076\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        all_losses: list\n",
    "            list that stores the values of the loss function (learning curve)\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        loss_avg = 0\n",
    "\n",
    "        for epoch in trange(1, n_iterations + 1, desc='Training in progress...'):\n",
    "            inp, target = data.random_training_set(batch_size)\n",
    "            loss = self.train_step(inp, target)\n",
    "            loss_avg += loss\n",
    "\n",
    "            if epoch % print_every == 0:\n",
    "                print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch,\n",
    "                                               epoch / n_iterations * 100, loss)\n",
    "                      )\n",
    "                #print(self.evaluate(data=data, prime_str='<',\n",
    "                #                    predict_len=100, batch_size=1), '\\n')\n",
    "\n",
    "            if epoch % plot_every == 0:\n",
    "                all_losses.append(loss_avg / plot_every)\n",
    "                loss_avg = 0\n",
    "        return all_losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
