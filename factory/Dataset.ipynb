{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a228691",
   "metadata": {},
   "source": [
    "# How do we process the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606e157",
   "metadata": {},
   "source": [
    "There are a few preset functions that already do this: the Dataloader in the Pytorch library, Keras also has it's dataloader function. However, during the course of the two use cases we explored above, we found out that we had to do extra preparation for loading. For asset price prediction, we did some extra preparation to put it into the Dataloader class from Pytorch, whereas for the quantum drug generator we instead used the dataloader function from the ReLeaSE algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8dbb66",
   "metadata": {},
   "source": [
    "What type of data do we work with?\n",
    "\n",
    "- Time series Data\n",
    "- Natural Language Processing\n",
    "- Image Captioning\n",
    "- Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0e4c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "from utils import time_since\n",
    "\n",
    "import pennylane as qml\n",
    "from quantum import QGRU\n",
    "\n",
    "#from smiles_enumerator import SmilesEnumerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fbee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[self.target].values).float()\n",
    "        self.X = torch.tensor(dataframe[self.features].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorData(object):\n",
    "    \"\"\"\n",
    "    Docstring coming soon...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_data_path, tokens=None, start_token='<',\n",
    "                 end_token='>', pad_symbol=' ', max_len=120, use_cuda=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor for the GeneratorData object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data_path: str\n",
    "            path to file with training dataset. Training dataset must contain\n",
    "            a column with training strings. The file also may contain other\n",
    "            columns.\n",
    "\n",
    "        tokens: list (default None)\n",
    "            list of characters specifying the language alphabet. Of left\n",
    "            unspecified, tokens will be extracted from data automatically.\n",
    "\n",
    "        start_token: str (default '<')\n",
    "            special character that will be added to the beginning of every\n",
    "            sequence and encode the sequence start.\n",
    "\n",
    "        end_token: str (default '>')\n",
    "            special character that will be added to the end of every\n",
    "            sequence and encode the sequence end.\n",
    "\n",
    "        max_len: int (default 120)\n",
    "            maximum allowed length of the sequences. All sequences longer than\n",
    "            max_len will be excluded from the training data.\n",
    "\n",
    "        use_cuda: bool (default None)\n",
    "            parameter specifying if GPU is used for computations. If left\n",
    "            unspecified, GPU will be used if available\n",
    "\n",
    "        kwargs: additional positional arguments\n",
    "            These include cols_to_read (list, default [0]) specifying which\n",
    "            column in the file with training data contains training sequences\n",
    "            and delimiter (str, default ',') that will be used to separate\n",
    "            columns if there are multiple of them in the file.\n",
    "\n",
    "        \"\"\"\n",
    "        super(GeneratorData, self).__init__()\n",
    "\n",
    "        if 'cols_to_read' not in kwargs:\n",
    "            kwargs['cols_to_read'] = []\n",
    "\n",
    "        data = read_object_property_file(training_data_path,\n",
    "                                         **kwargs)\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.file = []\n",
    "        for i in range(len(data)):\n",
    "            if len(data[i]) <= max_len:\n",
    "                self.file.append(self.start_token + data[i] + self.end_token)\n",
    "        self.file_len = len(self.file)\n",
    "        self.tokens = tokens\n",
    "        self.all_characters, self.char2idx, \\\n",
    "        self.n_characters = tokenize(self.file, tokens)\n",
    "        self.pad_symbol_idx = self.all_characters.index(self.pad_symbol)\n",
    "        self.use_cuda = use_cuda\n",
    "        if self.use_cuda is None:\n",
    "            self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    def load_dictionary(self, tokens, char2idx):\n",
    "        self.all_characters = tokens\n",
    "        self.char2idx = char2idx\n",
    "        self.n_characters = len(tokens)\n",
    "\n",
    "    def random_chunk(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples random SMILES string from generator training data set.\n",
    "        Returns:\n",
    "            random_smiles (str).\n",
    "        \"\"\"\n",
    "        index = np.random.randint(0, self.file_len - 1, batch_size)\n",
    "        return [self.file[i][:-1] for i in index], \\\n",
    "               [self.file[i][1:] for i in index]\n",
    "\n",
    "    def seq2tensor(self, seqs, tokens, flip=True):\n",
    "        tensor = np.zeros((len(seqs), len(seqs[0])))\n",
    "        for i in range(len(seqs)):\n",
    "            for j in range(len(seqs[i])):\n",
    "                if seqs[i][j] in tokens:\n",
    "                    tensor[i, j] = int(tokens.index(seqs[i][j]))\n",
    "                else:\n",
    "                    tokens = tokens + [seqs[i][j]]\n",
    "                    tensor[i, j] = int(tokens.index(seqs[i][j]))\n",
    "        if flip:\n",
    "            tensor = np.flip(tensor, axis=1).copy()\n",
    "        return tensor, tokens\n",
    "\n",
    "    def pad_sequences(self, seqs, max_length=None, pad_symbol=' '):\n",
    "        if max_length is None:\n",
    "            max_length = -1\n",
    "            for seq in seqs:\n",
    "                max_length = max(max_length, len(seq))\n",
    "        lengths = []\n",
    "        for i in range(len(seqs)):\n",
    "            cur_len = len(seqs[i])\n",
    "            lengths.append(cur_len)\n",
    "            seqs[i] = seqs[i] + pad_symbol * (max_length - cur_len)\n",
    "        return seqs, lengths\n",
    "\n",
    "    def random_training_set(self, batch_size):\n",
    "        inp, target = self.random_chunk(batch_size)\n",
    "        inp_padded, _ = self.pad_sequences(inp, pad_symbol=self.pad_symbol)\n",
    "        inp_tensor, self.all_characters = self.seq2tensor(inp_padded,\n",
    "                                                          tokens=self.all_characters,\n",
    "                                                          flip=False)\n",
    "        target_padded, _ = self.pad_sequences(target, pad_symbol=self.pad_symbol)\n",
    "        target_tensor, self.all_characters = self.seq2tensor(target_padded,\n",
    "                                                             tokens=self.all_characters,\n",
    "                                                             flip=False)\n",
    "        self.n_characters = len(self.all_characters)\n",
    "        inp_tensor = torch.tensor(inp_tensor).long()\n",
    "        target_tensor = torch.tensor(target_tensor).long()\n",
    "        if self.use_cuda:\n",
    "            inp_tensor = inp_tensor.cuda()\n",
    "            target_tensor = target_tensor.cuda()\n",
    "        return inp_tensor, target_tensor\n",
    "\n",
    "    def read_sdf_file(self, path, fields_to_read):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_data(self, path):\n",
    "        self.file, success = read_smi_file(path, unique=True)\n",
    "        self.file_len = len(self.file)\n",
    "        assert success\n",
    "        \n",
    "    def char_tensor(self, string):\n",
    "        \"\"\"\n",
    "        Converts SMILES into tensor of indices wrapped into torch.autograd.Variable.\n",
    "        Args:\n",
    "            string (str): input SMILES string\n",
    "        Returns:\n",
    "            tokenized_string (torch.autograd.Variable(torch.tensor))\n",
    "        \"\"\"\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            tensor[c] = self.all_characters.index(string[c])\n",
    "        if self.use_cuda:\n",
    "            return torch.tensor(tensor).cuda()\n",
    "        else:\n",
    "            return torch.tensor(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86d0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
